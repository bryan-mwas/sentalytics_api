{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>neutral</td>\n",
       "      <td>where in Eldoret can I go pick now ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>negative</td>\n",
       "      <td>Are all purchases to be prepaid? I have attem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1236</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Hey...308694786. Kindly deliver this today wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1750</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Does Jumia sell TVs with inbuilt decorder such...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>negative</td>\n",
       "      <td>And we might consider shifting shop</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      polarity                                               text\n",
       "749    neutral               where in Eldoret can I go pick now ?\n",
       "192   negative   Are all purchases to be prepaid? I have attem...\n",
       "1236   neutral   Hey...308694786. Kindly deliver this today wi...\n",
       "1750   neutral  Does Jumia sell TVs with inbuilt decorder such...\n",
       "110   negative                And we might consider shifting shop"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "path = 'dataset/labelled_tweets.csv'\n",
    "    \n",
    "tweet = pd.read_csv(path, usecols=['text', 'polarity'])\n",
    "# Remove rows will nan values\n",
    "tweet = tweet.dropna()\n",
    "tweet = tweet.reindex(np.random.permutation(tweet.index))\n",
    "# preprocessing bit\n",
    "# replace user handles (@Jumia) to be empty\n",
    "pattern = \"(@[A-Za-z0-9]+)|(http|https|ftp)://[a-zA-Z0-9./]+|#(\\w+)\"\n",
    "tweet['text'] = tweet.text.str.replace(pattern, '')\n",
    "tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral     959\n",
       "negative    748\n",
       "positive    100\n",
       "Name: polarity, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine class distribution\n",
    "tweet.polarity.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert label to a numerical count by creating a new column\n",
    "tweet['polarity_num'] = tweet.polarity.map({'negative':0,'positive':1,'neutral':2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1807,)\n",
      "(1807,)\n"
     ]
    }
   ],
   "source": [
    "# Define X matrix as features and y as vectors\n",
    "X = tweet.text\n",
    "y = tweet.polarity_num\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1355,)\n",
      "(452,)\n",
      "(1355,)\n",
      "(452,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_nb = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', MultinomialNB())])\n",
    "clf_nb = clf_nb.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred_class' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-7b2dc0858b54>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred_class\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'y_pred_class' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "np.mean(y_pred_class == y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_svm = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', SGDClassifier(loss='hinge',penalty='l2',alpha=1e-3, n_iter=5,random_state=42))])\n",
    "clf_svm = clf_svm.fit(X_train,y_train)\n",
    "y_pred_class = clf_svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf_reg = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', LogisticRegression())])\n",
    "clf_reg = clf_reg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV \n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)], 'tfidf__use_idf': (True, False), }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_list = [clf_nb, clf_reg, clf_svm]\n",
    "clf_dict = {\n",
    "    clf_nb : {'vect__ngram_range': [(1, 1), (1, 2)],'tfidf__use_idf': (True, False),'clf__alpha': (1e-2, 1e-3),},\n",
    "    clf_reg : {'vect__ngram_range': [(1, 1), (1, 2)], 'tfidf__use_idf': (True, False), },\n",
    "    clf_svm : {'vect__ngram_range': [(1, 1), (1, 2)],'tfidf__use_idf': (True, False),'clf__alpha': (1e-2, 1e-3),}\n",
    "}\n",
    "\n",
    "scores = {}\n",
    "# loop every key and value in the dictionary\n",
    "for clf, params in clf_dict.items():\n",
    "    gs_clf = GridSearchCV(clf, params, n_jobs=-1, cv=10)\n",
    "    gs_clf = gs_clf.fit(X_train, y_train)\n",
    "    scores[clf] = gs_clf.best_score_\n",
    "\n",
    "\n",
    "\n",
    "# for clf in clf_list:\n",
    "#     gs_clf = GridSearchCV(clf, parameters, n_jobs=-1, cv=10)\n",
    "#     gs_clf = gs_clf.fit(X_train, y_train)\n",
    "#     print(gs_clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle the scores for faster access\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(scores, 'scores.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = joblib.load('scores.pkl')\n",
    "maximum = max(scores, key=scores.get)  # Just use 'min' instead of 'max' for minimum.\n",
    "print(maximum, scores[maximum])\n",
    "# scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate accuracy of class predictions\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "metrics.accuracy_score(y_test, y_pred_class)\n",
    "print(classification_report(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vect = CountVectorizer()\n",
    "X_dtm = vect.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "logreg = MultinomialNB()\n",
    "print(cross_val_score(logreg, X_dtm, y, cv=10, scoring='accuracy').mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This snippet of code was extracted from: \n",
    "http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\"\"\"\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "class_names = ['negative','positive','neutral']\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred_class)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_svm.predict(['OK'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brian\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "order placed confirm cancelled happened need hello want received week\n",
      "Topic 1:\n",
      "thank yes response alright sorted received assistance recieved jamo customer\n",
      "Topic 2:\n",
      "com twitter pic https jumiakenya email time money ready ago\n",
      "Topic 3:\n",
      "thanks alright got sorted great assisted lot yeah received wanted\n",
      "Topic 4:\n",
      "dm follow send mind like following ur respond great did\n",
      "Topic 5:\n",
      "cancel like order want replaced need orders just does yes\n",
      "Topic 6:\n",
      "delivery pay option hello expect monday today cash saturday called\n",
      "Topic 7:\n",
      "okay week boss look past cheers tomorrow expensive days price\n",
      "Topic 8:\n",
      "waiting reply feedback ll weeks tomorrow need ago today sawa\n",
      "Topic 9:\n",
      "kindly confirm advise update assist send advice help look item\n",
      "Topic 10:\n",
      "delivered today ordered package said tomorrow paid item want ada\n",
      "Topic 11:\n",
      "number order got 304684986 paybill thats confirm 301131586 ordered provided\n",
      "Topic 12:\n",
      "long wait days taking deliver 308891886 refund product package guys\n",
      "Topic 13:\n",
      "ok poor fine ready pick provide looking paybill hope forward\n",
      "Topic 14:\n",
      "hi need customer yesterday package like pick received item recieved\n",
      "Topic 15:\n",
      "phone bought infinix ordered received warranty problem mobile charging going\n",
      "Topic 16:\n",
      "check email dm guys address placed 308576386 tomorrow kindly morning\n",
      "Topic 17:\n",
      "status update hey whats current 301131586 confirm guys order hello\n",
      "Topic 18:\n",
      "items need locally different new site shipped available cancelled update\n",
      "Topic 19:\n",
      "jumia hello ke item week placed friday ordered need arrived\n",
      "Topic 0:\n",
      "guys phones sell instead sent waiting wireless message apology understand\n",
      "Topic 1:\n",
      "bought infinix need hot hi phone problem order link jumia\n",
      "Topic 2:\n",
      "dm follow pick today like nkt offices customers response bring\n",
      "Topic 3:\n",
      "waiting price current sawa product pixi really inch lady talked\n",
      "Topic 4:\n",
      "order delivery kindly number delivered cancel hi days received item\n",
      "Topic 5:\n",
      "friday hello buying unga access saturday monday collected 306593986 apparently\n",
      "Topic 6:\n",
      "just money does did need patient support id maybe info\n",
      "Topic 7:\n",
      "contact tried used using laptop agents inquire purchasing trying urgent\n",
      "Topic 8:\n",
      "share noted jamo expensive gopro 0716575105 button requested 350mbs working\n",
      "Topic 9:\n",
      "payment online code numbers sale site jumia answered confirmation clear\n",
      "Topic 10:\n",
      "com twitter pic https email phone jumiakenya wapi uko ni\n",
      "Topic 11:\n",
      "black jumia watch sim dual details ram ke camera protector\n",
      "Topic 12:\n",
      "buy fee delivery 300 available 350 plus tecno tv afternoon\n",
      "Topic 13:\n",
      "customer kindly care refund check issue experience great order services\n",
      "Topic 14:\n",
      "1st means caro sleek request smart buy ordered parcel flash\n",
      "Topic 15:\n",
      "phone time bought nairobi disappointed working warranty items return replace\n",
      "Topic 16:\n",
      "service office long number center month just pay posta post\n",
      "Topic 17:\n",
      "thanks okay sorted contacted ok use alright voucher ll feedback\n",
      "Topic 18:\n",
      "thank ok don make hope right yes think delivery let\n",
      "Topic 19:\n",
      "302764996 tena boss copy 303594276 news created provided okay thank\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_index, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_index))\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words -1:-1]]))\n",
    "\n",
    "# NMF is able to use tf-idf\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(X)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(X)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "no_topics = 20\n",
    "\n",
    "# Run NMF\n",
    "nmf = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)\n",
    "\n",
    "# Run LDA\n",
    "lda = LatentDirichletAllocation(n_topics=no_topics, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(tf)\n",
    "\n",
    "no_top_words = 10\n",
    "display_topics(nmf, tfidf_feature_names, no_top_words)\n",
    "display_topics(lda, tf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 1000)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brian\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "order cancel number kindly\n",
      " today is my birthday..\n",
      "Can i get one?\n",
      " hi.Kindly update me on rectification of a return of a product made recently after consulting customer care.Order no 307245886\n",
      "why has it taken this long when my previous orders took only a few days??\n",
      "Topic 1:\n",
      "thank okay ok yes\n",
      "Okay thanks I will\n",
      " its now 11 days since I placed an order and paid for a phone,order no.303673286..up to now I have not received it?..wassup?\n",
      "Thank you\n",
      " why did you supply me one for UAE knowing very well am in Kenya. That is conmanship\n",
      "Topic 0:\n",
      "order delivery number thanks\n",
      " hi. My order was cancelled and I still received a message saying that it was ready for collection. Kindly advise.\n",
      "I want to buy a phone cover but it's from Jumia Global. Delivery is at Adams Arcade pick up station\n",
      " ORDER N.° 30429438 i have picked my package the phone is okay but the flash disc isnt functioning\n",
      " i wanna buy a phone from you. where are ur offices?\n",
      "Topic 1:\n",
      "order waiting phone delivered\n",
      "   _DUKAS https:// twitter.com/Ogungaken/stat us/635386818259746820 …\n",
      " I've been instructed to pick my package at the CBD yet that is not my location and you have charged delivery. Why?\n",
      "LHH7W7U4I1 Confirmed. Ksh4,500.00 sent to JOHN ITHARUNI 0723334065 on 17/8/17 at 4:24 PM. New M-PESA balance is Ksh...\n",
      "Hey how much does oppo f1 plus cost in jumia\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "import numpy as np\n",
    "\n",
    "def display_topics(H, W, feature_names, documents, no_top_words, no_top_documents):\n",
    "    for topic_idx, topic in enumerate(H):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "        top_doc_indices = np.argsort( W[:,topic_idx] )[::-1][0:no_top_documents]\n",
    "        for doc_index in top_doc_indices:\n",
    "            print(documents[doc_index])\n",
    "\n",
    "# NMF is able to use tf-idf\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(X)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(X)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "no_topics = 2\n",
    "\n",
    "# Run NMF\n",
    "nmf_model = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)\n",
    "nmf_W = nmf_model.transform(tfidf)\n",
    "nmf_H = nmf_model.components_\n",
    "\n",
    "# Run LDA\n",
    "lda_model = LatentDirichletAllocation(n_topics=no_topics, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(tf)\n",
    "lda_W = lda_model.transform(tf)\n",
    "lda_H = lda_model.components_\n",
    "\n",
    "no_top_words = 4\n",
    "no_top_documents = 4\n",
    "display_topics(nmf_H, nmf_W, tfidf_feature_names, X, no_top_words, no_top_documents)\n",
    "display_topics(lda_H, lda_W, tf_feature_names, X, no_top_words, no_top_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
